{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Housing Price Prediction - Model Training & YAML Export\n",
                "\n",
                "This notebook trains a Random Forest model for housing price prediction and exports the configuration to YAML format for deployment with Streamlit."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                "import joblib\n",
                "import yaml\n",
                "from datetime import datetime\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the training data\n",
                "print(\"Loading data...\")\n",
                "train_data = pd.read_csv('train_100k.csv')\n",
                "\n",
                "# Display basic info\n",
                "print(f\"\\nTraining data shape: {train_data.shape}\")\n",
                "print(f\"\\nColumns: {list(train_data.columns)}\")\n",
                "train_data.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data info\n",
                "train_data.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "train_data.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify target column\n",
                "target_cols = [col for col in train_data.columns if 'price' in col.lower() or 'sale' in col.lower()]\n",
                "if target_cols:\n",
                "    target_column = target_cols[0]\n",
                "else:\n",
                "    target_column = train_data.columns[-1]\n",
                "\n",
                "print(f\"Target column: {target_column}\")\n",
                "\n",
                "# Separate features and target\n",
                "X = train_data.drop(columns=[target_column])\n",
                "y = train_data[target_column]\n",
                "\n",
                "print(f\"\\nFeatures shape: {X.shape}\")\n",
                "print(f\"Target shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify categorical and numerical columns\n",
                "categorical_cols = X.select_dtypes(include=['object']).columns\n",
                "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
                "\n",
                "print(f\"Categorical columns ({len(categorical_cols)}): {list(categorical_cols)}\")\n",
                "print(f\"\\nNumerical columns ({len(numerical_cols)}): {list(numerical_cols)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "missing_values = X.isnull().sum()\n",
                "if missing_values.sum() > 0:\n",
                "    print(\"Missing values:\")\n",
                "    print(missing_values[missing_values > 0])\n",
                "else:\n",
                "    print(\"No missing values found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# One-hot encode categorical variables\n",
                "print(\"Encoding categorical variables...\")\n",
                "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
                "\n",
                "# Handle missing values\n",
                "X_encoded = X_encoded.fillna(X_encoded.median())\n",
                "\n",
                "print(f\"\\nEncoded features shape: {X_encoded.shape}\")\n",
                "print(f\"Total features after encoding: {len(X_encoded.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_encoded, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape}\")\n",
                "print(f\"Test set: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"Features scaled using StandardScaler\")\n",
                "print(f\"Mean of scaled training data: {X_train_scaled.mean():.6f}\")\n",
                "print(f\"Std of scaled training data: {X_train_scaled.std():.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Random Forest model\n",
                "print(\"Training Random Forest model...\\n\")\n",
                "\n",
                "model = RandomForestRegressor(\n",
                "    n_estimators=100,\n",
                "    max_depth=20,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "model.fit(X_train_scaled, y_train)\n",
                "print(\"\\n‚úì Model training completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "y_pred = model.predict(X_test_scaled)\n",
                "\n",
                "# Calculate metrics\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "rmse = np.sqrt(mse)\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"MODEL PERFORMANCE METRICS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Root Mean Squared Error (RMSE): ${rmse:,.2f}\")\n",
                "print(f\"Mean Absolute Error (MAE):      ${mae:,.2f}\")\n",
                "print(f\"R¬≤ Score:                       {r2:.4f}\")\n",
                "print(f\"Accuracy:                       {r2*100:.2f}%\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictions vs actual\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.scatter(y_test, y_pred, alpha=0.5)\n",
                "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
                "plt.xlabel('Actual Price ($)')\n",
                "plt.ylabel('Predicted Price ($)')\n",
                "plt.title('Actual vs Predicted Prices')\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "residuals = y_test - y_pred\n",
                "plt.scatter(y_pred, residuals, alpha=0.5)\n",
                "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
                "plt.xlabel('Predicted Price ($)')\n",
                "plt.ylabel('Residuals ($)')\n",
                "plt.title('Residual Plot')\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X_train.columns,\n",
                "    'importance': model.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"\\nTop 15 Most Important Features:\")\n",
                "print(\"=\"*60)\n",
                "for idx, row in feature_importance.head(15).iterrows():\n",
                "    print(f\"{row['feature']:30s} {row['importance']*100:6.2f}%\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize top 20 features\n",
                "plt.figure(figsize=(12, 8))\n",
                "top_features = feature_importance.head(20)\n",
                "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
                "plt.yticks(range(len(top_features)), top_features['feature'])\n",
                "plt.xlabel('Feature Importance')\n",
                "plt.title('Top 20 Most Important Features')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.grid(True, alpha=0.3, axis='x')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Model and Export to YAML"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the model and scaler\n",
                "print(\"Saving model artifacts...\")\n",
                "joblib.dump(model, 'housing_price_model.pkl')\n",
                "joblib.dump(scaler, 'scaler.pkl')\n",
                "\n",
                "# Save feature names\n",
                "with open('feature_names.json', 'w') as f:\n",
                "    json.dump(list(X_train.columns), f)\n",
                "\n",
                "print(\"‚úì Model saved to: housing_price_model.pkl\")\n",
                "print(\"‚úì Scaler saved to: scaler.pkl\")\n",
                "print(\"‚úì Feature names saved to: feature_names.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create YAML configuration\n",
                "model_config = {\n",
                "    'model_info': {\n",
                "        'name': 'Housing Price Prediction Model',\n",
                "        'type': 'RandomForestRegressor',\n",
                "        'version': '1.0',\n",
                "        'created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
                "        'description': 'Random Forest model for predicting housing prices'\n",
                "    },\n",
                "    'hyperparameters': {\n",
                "        'n_estimators': 100,\n",
                "        'max_depth': 20,\n",
                "        'min_samples_split': 5,\n",
                "        'min_samples_leaf': 2,\n",
                "        'random_state': 42\n",
                "    },\n",
                "    'data_info': {\n",
                "        'training_samples': len(X_train),\n",
                "        'test_samples': len(X_test),\n",
                "        'total_features': len(X_train.columns),\n",
                "        'categorical_features': len(categorical_cols),\n",
                "        'numerical_features': len(numerical_cols),\n",
                "        'target_column': target_column\n",
                "    },\n",
                "    'performance_metrics': {\n",
                "        'rmse': float(rmse),\n",
                "        'mae': float(mae),\n",
                "        'r2_score': float(r2),\n",
                "        'accuracy_percentage': float(r2 * 100)\n",
                "    },\n",
                "    'feature_importance': {\n",
                "        row['feature']: float(row['importance']) \n",
                "        for _, row in feature_importance.head(20).iterrows()\n",
                "    },\n",
                "    'preprocessing': {\n",
                "        'scaling': 'StandardScaler',\n",
                "        'categorical_encoding': 'One-Hot Encoding',\n",
                "        'missing_value_strategy': 'Median Imputation'\n",
                "    },\n",
                "    'files': {\n",
                "        'model_file': 'housing_price_model.pkl',\n",
                "        'scaler_file': 'scaler.pkl',\n",
                "        'feature_names_file': 'feature_names.json'\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save YAML configuration\n",
                "print(\"\\nExporting model configuration to YAML...\")\n",
                "with open('model_config.yaml', 'w') as f:\n",
                "    yaml.dump(model_config, f, default_flow_style=False, sort_keys=False)\n",
                "\n",
                "print(\"‚úì Configuration saved to: model_config.yaml\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the YAML configuration\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"YAML CONFIGURATION PREVIEW\")\n",
                "print(\"=\"*60)\n",
                "print(yaml.dump(model_config, default_flow_style=False, sort_keys=False))\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úì MODEL TRAINING AND EXPORT COMPLETED!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nüìä Model Performance:\")\n",
                "print(f\"   - Accuracy: {r2*100:.2f}%\")\n",
                "print(f\"   - RMSE: ${rmse:,.2f}\")\n",
                "print(f\"   - MAE: ${mae:,.2f}\")\n",
                "print(f\"\\nüìÅ Generated Files:\")\n",
                "print(f\"   - housing_price_model.pkl (trained model)\")\n",
                "print(f\"   - scaler.pkl (feature scaler)\")\n",
                "print(f\"   - feature_names.json (feature list)\")\n",
                "print(f\"   - model_config.yaml (configuration)\")\n",
                "print(f\"\\nüöÄ Next Steps:\")\n",
                "print(f\"   Run the Streamlit app: streamlit run app.py\")\n",
                "print(f\"   Open browser at: http://localhost:8501\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Test Predictions (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data and make predictions\n",
                "try:\n",
                "    test_data = pd.read_csv('test_100k.csv')\n",
                "    print(f\"Loaded test data: {test_data.shape}\")\n",
                "    \n",
                "    # Preprocess test data\n",
                "    X_test_encoded = pd.get_dummies(test_data)\n",
                "    \n",
                "    # Align columns with training data\n",
                "    missing_cols = set(X_train.columns) - set(X_test_encoded.columns)\n",
                "    for col in missing_cols:\n",
                "        X_test_encoded[col] = 0\n",
                "    \n",
                "    X_test_encoded = X_test_encoded[X_train.columns]\n",
                "    X_test_encoded = X_test_encoded.fillna(X_test_encoded.median())\n",
                "    \n",
                "    # Scale and predict\n",
                "    X_test_scaled = scaler.transform(X_test_encoded)\n",
                "    test_predictions = model.predict(X_test_scaled)\n",
                "    \n",
                "    # Create results dataframe\n",
                "    results = pd.DataFrame({\n",
                "        'Id': range(len(test_predictions)),\n",
                "        'Predicted_Price': test_predictions\n",
                "    })\n",
                "    \n",
                "    print(f\"\\n‚úì Generated {len(test_predictions):,} predictions\")\n",
                "    print(f\"\\nPrediction Statistics:\")\n",
                "    print(f\"   Average: ${test_predictions.mean():,.2f}\")\n",
                "    print(f\"   Median:  ${np.median(test_predictions):,.2f}\")\n",
                "    print(f\"   Min:     ${test_predictions.min():,.2f}\")\n",
                "    print(f\"   Max:     ${test_predictions.max():,.2f}\")\n",
                "    \n",
                "    # Save predictions\n",
                "    results.to_csv('housing_price_predictions.csv', index=False)\n",
                "    print(f\"\\n‚úì Predictions saved to: housing_price_predictions.csv\")\n",
                "    \n",
                "    # Display first few predictions\n",
                "    print(\"\\nFirst 10 predictions:\")\n",
                "    display(results.head(10))\n",
                "    \n",
                "except FileNotFoundError:\n",
                "    print(\"Test file 'test_100k.csv' not found. Skipping test predictions.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}